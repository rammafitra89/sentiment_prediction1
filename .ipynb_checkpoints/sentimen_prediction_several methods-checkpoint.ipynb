{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>datasetsplit</th>\n",
       "      <th>sentimenvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Effective but too-tepid biopic</td>\n",
       "      <td>2</td>\n",
       "      <td>0.44444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.42708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11851</td>\n",
       "      <td>A real snooze .</td>\n",
       "      <td>1</td>\n",
       "      <td>0.44444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11852</td>\n",
       "      <td>No surprises .</td>\n",
       "      <td>1</td>\n",
       "      <td>0.19444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11853</td>\n",
       "      <td>We 've seen the hippie-turned-yuppie plot befo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.61111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11854</td>\n",
       "      <td>Her fans walked out muttering words like `` ho...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11855</td>\n",
       "      <td>In this case zero .</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11855 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentences datasetsplit  \\\n",
       "1      The Rock is destined to be the 21st Century 's...            1   \n",
       "2      The gorgeously elaborate continuation of `` Th...            1   \n",
       "3                         Effective but too-tepid biopic            2   \n",
       "4      If you sometimes like to go to the movies to h...            2   \n",
       "5      Emerges as something rare , an issue movie tha...            2   \n",
       "...                                                  ...          ...   \n",
       "11851                                    A real snooze .            1   \n",
       "11852                                     No surprises .            1   \n",
       "11853  We 've seen the hippie-turned-yuppie plot befo...            1   \n",
       "11854  Her fans walked out muttering words like `` ho...            1   \n",
       "11855                                In this case zero .            1   \n",
       "\n",
       "      sentimenvalue  \n",
       "1               0.5  \n",
       "2               0.5  \n",
       "3           0.44444  \n",
       "4               0.5  \n",
       "5           0.42708  \n",
       "...             ...  \n",
       "11851       0.44444  \n",
       "11852       0.19444  \n",
       "11853       0.61111  \n",
       "11854       0.72222  \n",
       "11855           0.5  \n",
       "\n",
       "[11855 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import library pandas\n",
    "import pandas as pd\n",
    "# call the standfordsentimenttreesbank\n",
    "data1= pd.read_csv('datasetSentences.txt',sep ='\\t', header=None) \n",
    "data3= pd.read_csv('datasetSplit.txt',sep =',', header=None)\n",
    "data5 = pd.read_csv('sentiment_labels.txt',sep ='|', header=None)\n",
    "#delete column 1 in every column data\n",
    "data2 = data1.drop(0,1)\n",
    "data4 = data3.drop(0,1)\n",
    "data6 = data5.drop(0,1)\n",
    "#change column name in every column\n",
    "data2.columns =['sentences']\n",
    "data4.columns =['datasplit']\n",
    "data6.columns =['Sentiment_value']\n",
    "#delete first raw in every column\n",
    "data2_drop = data2.drop(data2.index[0])\n",
    "data4_drop = data4.drop(data4.index[0])\n",
    "data6_drop = data6.drop(data4.index[0])\n",
    "#merge all columns\n",
    "data2['datasetsplit'] = data4_drop\n",
    "data2_drop= data2.drop(data2.index[0])\n",
    "data2_drop['sentimenvalue'] = data6_drop\n",
    "#initialize data2_drop to data\n",
    "data = data2_drop\n",
    "#print data \n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengimpor library\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    " \n",
    "# Mengimpor dataset\n",
    "dataset = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Line 10-34 adalah proses yang dilakukan setahap demi setahap (agar mudah dipahami)\n",
    "# Melihat item pertama di dataset\n",
    "dataset['sentences'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengimpor library re dan NLTK\n",
    "import re\n",
    "import nltk\n",
    "sentences = re.sub('[^a-zA-Z]', ' ', dataset['sentences'][1])\n",
    "sentences = sentences.lower()\n",
    "sentences = sentences.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -3]\n",
      "[nltk_data]     Temporary failure in name resolution>\n"
     ]
    }
   ],
   "source": [
    "# Mendownload daftar kata yang ada (vocabulary)\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memeriksa daftar kata di stopwords\n",
    "inggris = stopwords.words('english')\n",
    "indo = stopwords.words('indonesian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghilangkan kata yang tidak ada di stopwords\n",
    "sentences = [word for word in sentences if not word in inggris]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melakukan proses stemming (penggunaan kata dasar)\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membersihkan kalimat dari kata yang ada di stopwords\n",
    "sentences = [ps.stem(word) for word in sentences if not word in inggris]\n",
    "sentences = ' '.join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -3]\n",
      "[nltk_data]     Temporary failure in name resolution>\n"
     ]
    }
   ],
   "source": [
    "# Melakukan proses cleaning pada teks\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "corpus = []\n",
    "for i in range(1, len(dataset)):\n",
    "    sentences= re.sub('[^a-zA-Z]', ' ', dataset['sentences'][i])\n",
    "    sentences= sentences.lower()\n",
    "    sentences = sentences.split()\n",
    "    ps = PorterStemmer()\n",
    "    sentences = [ps.stem(word) for word in sentences if not word in inggris]\n",
    "    sentences = ' '.join(sentences)\n",
    "    corpus.append(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat model Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[1:, 1].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membagi dataset ke dalam Training dan Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rama/.local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/rama/.local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Menggunakan beberapa teknik klasifikasi untuk membandingkan akurasinya\n",
    "# Metode Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifierLR = LogisticRegression(random_state = 0)\n",
    "classifierLR.fit(X_train, y_train)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metode K-nearest Neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifierKNN = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "classifierKNN.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metode SVM\n",
    "from sklearn.svm import SVC\n",
    "classifierSVM = SVC(kernel = 'linear', random_state = 0)\n",
    "classifierSVM.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metode Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifierNB = GaussianNB()\n",
    "classifierNB.fit(X_train, y_train)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metode Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifierDT = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifierDT.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metode Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifierRF = RandomForestClassifier(n_estimators = 500, criterion = 'entropy', random_state = 0)\n",
    "classifierRF.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memprediksi hasil Test Set\n",
    "y_pred_LR = classifierLR.predict(X_test)    # logistic Regression\n",
    "y_pred_KNN = classifierKNN.predict(X_test)  # K-nearest Neighbors\n",
    "y_pred_SVM = classifierSVM.predict(X_test)  # SVM\n",
    "y_pred_NB = classifierNB.predict(X_test)    # Naive Bayes\n",
    "y_pred_DT = classifierDT.predict(X_test)    # Decision Tree\n",
    "y_pred_RF = classifierRF.predict(X_test)    # Random Forest\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_LR = confusion_matrix(y_test, y_pred_LR)\n",
    "cm_KNN = confusion_matrix(y_test, y_pred_KNN)\n",
    "cm_SVM = confusion_matrix(y_test, y_pred_SVM)\n",
    "cm_NB = confusion_matrix(y_test, y_pred_NB)\n",
    "cm_DT = confusion_matrix(y_test, y_pred_DT)\n",
    "cm_RF = confusion_matrix(y_test, y_pred_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menilai akurasi masing-masing metode\n",
    "akurasi_LR = ((cm_LR[0][0]+cm_LR[1][1])/(cm_LR[0][0]+cm_LR[1][1]+cm_LR[0][1]+cm_LR[1][0]))*100\n",
    "akurasi_KNN = ((cm_KNN[0][0]+cm_KNN[1][1])/(cm_KNN[0][0]+cm_KNN[1][1]+cm_KNN[0][1]+cm_KNN[1][0]))*100\n",
    "akurasi_SVM = ((cm_SVM[0][0]+cm_SVM[1][1])/(cm_SVM[0][0]+cm_SVM[1][1]+cm_SVM[0][1]+cm_SVM[1][0]))*100\n",
    "akurasi_NB = ((cm_NB[0][0]+cm_NB[1][1])/(cm_NB[0][0]+cm_NB[1][1]+cm_NB[0][1]+cm_NB[1][0]))*100\n",
    "akurasi_DT = ((cm_DT[0][0]+cm_DT[1][1])/(cm_DT[0][0]+cm_DT[1][1]+cm_DT[0][1]+cm_DT[1][0]))*100\n",
    "akurasi_RF = ((cm_RF[0][0]+cm_RF[1][1])/(cm_RF[0][0]+cm_RF[1][1]+cm_RF[0][1]+cm_RF[1][0]))*100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
